# SARSA
SARSA is an on-control temporal difference algorithm that uses bootstrapping and experience at the subsequent state to determine the currrent estimate. In contrast to Q-learning, it being on-control means the control and target policies are the same, i.e., the policy used to select behaviours and the one being evaluated and improved are not different. Moreover, the action-value at the subsequent state is not determined by taking a max over all possible action-values at that state, but is chosen according to an $\epsilon$-greedy selection for the policy, and is then for the update. Its formula is the following:

$$Q(s, a) \gets Q(s, a) + \alpha (r + \gamma Q(s', a') - Q(s, a))$$

In terms of the environment, CliffWalker was chosen. It is a standard gridworld with a cliff at the bottom that leads to a reward of -100. In contrast to FrozenLake, every time step has a reward of -1 until you reach the target state. Moreover, the environment is deterministic, not stochastic. Another important difference between Sarsa and Q-learning are the paths taken; Q-learning is far more aggressive in risk-taking and takes the shortest possible path around and touching the cliff to get to the target, while Sarsa is more conservative and takes a more roundabout but safer way. 
